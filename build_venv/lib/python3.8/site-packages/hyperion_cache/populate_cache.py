"""Script that populates all of the cache files"""

from argparse import ArgumentParser
import base64
import json
from os import path, mkdir
from time import time
from typing import Callable

from requests import Session
from requests.exceptions import HTTPError


CACHE_ROOT = {'path': path.join(path.dirname(path.realpath(__file__)), 'cache-files')}  # Is dict for testing

# Although some would argue we should get rid of Windows and not support it, this code is to support Windows.
# For example, item type sshkey has items named like ephemeral_keys:infra
# The colon is reserved in Windows and cannot be used in a folder or filename.
# List of reserved characters,
# https://docs.microsoft.com/en-us/windows/win32/fileio/naming-a-file?redirectedfrom=MSDN#file_and_directory_names
# To quote Nicholas Barber "Using a lambda is such a C# thing to do." We are doing it anyways.
ENCODE_STRING: Callable[[str], str] = lambda x: base64.b64encode(x.encode("utf-8"), b"-_").decode()

def get_all_records(session: Session, habitat: str) -> None:
    """
    Uses the metadata record to iterate over all hyperion records and store them in cache files
    :param session: The requests session to use when talking to hyperion
    :param habitat: The habitat to talk to hyperion in and write files for
    """
    now = time()
    hyperion_root_url = f'https://hyperion.prv-use1.{habitat}-pure.cloud/v1'
    metadata = session.get(url=f'{hyperion_root_url}/item_type/metadata/name/list', params={'ts': now})
    metadata.raise_for_status()
    habitat_cache = path.join(CACHE_ROOT['path'], habitat)
    if not path.exists(habitat_cache):
        mkdir(habitat_cache)
    for item_type, items in metadata.json()['content']['properties']['item_types'].items():
        print(f'\nGetting records for item type: {item_type}')
        item_type_cache = path.join(habitat_cache, item_type)
        if not path.exists(item_type_cache):
            mkdir(item_type_cache)
        with open(path.join(item_type_cache, 'metadata.json'), 'w', encoding='utf8') as metadata_json:
            json.dump({'content': items}, metadata_json)
        for item in items:
            print(f'Getting {item_type}: {item}')
            item_url = f'{hyperion_root_url}/item_type/{item_type}/name/{item}'
            try:
                base_item = session.get(url=item_url, params={'ts': now})
                base_item.raise_for_status()
            except HTTPError as ex:
                print(f'Request for {item_type}: {item} failed with {base_item.status_code} - {ex}')
                if 399 < base_item.status_code < 500:
                    print('4XX error code, continuing.')
                    continue
                print('Fatal error, raising.')
                raise
            expanded_item = session.get(url=f'{item_url}/expanded', params={'ts': now})
            expanded_item.raise_for_status()
            item_cache = path.join(item_type_cache, ENCODE_STRING(item))
            if not path.exists(item_cache):
                mkdir(item_cache)
            with open(path.join(item_cache, 'base.json'), 'w', encoding='utf8') as base_json:
                base_json.write(base_item.text)
            with open(path.join(item_cache, 'expanded.json'), 'w', encoding='utf8') as expanded_json:
                expanded_json.write(expanded_item.text)


def main() -> None:
    """Main entry point for the script"""
    parser = ArgumentParser()
    parser.add_argument('habitats', nargs='*', default=['dev', 'infra'])
    args = parser.parse_args()
    session = Session()
    for habitat in args.habitats:
        print(f'\nFetching for {habitat}')
        get_all_records(session=session, habitat=habitat)


if __name__ == '__main__':
    main()
