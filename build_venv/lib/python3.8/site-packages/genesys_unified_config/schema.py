"""Unified Config Schema"""
from typing import Any, Dict

import pytz
from jsonschema.exceptions import SchemaError
from jsonschema.validators import Draft202012Validator
from marshmallow import Schema, fields, validate, validates_schema, ValidationError, validates, post_load
from marshmallow.validate import OneOf, Range, Length
from marshmallow_polyfield import PolyField
from marshmallow_union import Union

valid_stigs = [
    '3.2.0',
    '2022.2.0/1',
]
valid_business_units = ['CX', 'DX', 'EX']
valid_product_groups = ['Platform', 'AI', 'WEM', 'Digital']

STATIC_ASSETS_SNS_PUSH = 'SNS-push'
STATIC_ASSETS_S3_COPY = 'S3-copy'
STATIC_ASSETS_ACTIONS = [STATIC_ASSETS_SNS_PUSH, STATIC_ASSETS_S3_COPY]
STATIC_ASSETS_SRC_TYPES = ['file', 'zip']

SERVICE_TESTS_UNIFIED = 'unified'
SERVICE_TESTS_LEGACY = 'legacy'

HABITAT_TYPE_DEPLOY = 'habitat'
ENVIRONMENT_TYPE_DEPLOY = 'environment'

VALID_SEQUENCE_TYPES = ['parallel', 'serial']
DEFAULT_SCHEDULE = 'always'
VALID_TYPES_FOR_PATH = ['habitat', 'environment']

CUSTOM_LAMBDA_ACCESS_TYPES = ['direct', 'indirect']
CLOUDFORMATION_NAME_SPACES = ['AWS', 'Genesys']
GENESYS_CLOUDFORMATION_TYPES = ['Genesys::EC2::InstanceId', 'Genesys::AutoScaling::AutoScalingGroupName']

DAYS_OF_WEEK = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']

SYSTEM_PORT_END = 1025
MAX_PORT = 65536

# Open Telemetry Constants
OPEN_TELEMETRY_MIN_MEMORY_MAX_PERCENTAGE = 10
OPEN_TELEMETRY_MAX_MEMORY_MAX_PERCENTAGE = 25

OPEN_TELEMETRY_MIN_MEMORY_SPIKE_PERCENTAGE = 9
OPEN_TELEMETRY_MAX_MEMORY_SPIKE_PERCENTAGE = 24

OPEN_TELEMETRY_DEFAULT_MEMORY_MAX_PERCENTAGE = 20
OPEN_TELEMETRY_DEFAULT_MEMORY_SPIKE_PERCENTAGE = 15

OPEN_TELEMETRY_DEFAULT_HTTP_COLLECTOR_PORT = 4318
OPEN_TELEMETRY_DEFAULT_COLLECTOR_METRICS_PORT = 8890

##################
# Weyland Config #
##################

# Taken from: https://semver.org/#is-there-a-suggested-regular-expression-regex-to-check-a-semver-string
SEM_VER_REGEX = r"^(?P<major>0|[1-9]\d*)\.(?P<minor>0|[1-9]\d*)\.(?P<patch>0|[1-9]\d*)(?:-(?P<prerelease>(?:0|[1-9]\d*|\d*[a-zA-Z-][0-9a-zA-Z-]*)(?:\.(?:0|[1-9]\d*|\d*[a-zA-Z-][0-9a-zA-Z-]*))*))?(?:\+(?P<buildmetadata>[0-9a-zA-Z-]+(?:\.[0-9a-zA-Z-]+)*))?$" # pylint: disable=line-too-long

valid_providers = ['pagerduty', 'newrelic', 'genesys', 'google', 'azurerm', 'azuread', 'sumologic']
##################

class Hyperlink(Schema):
    """Schema for hyperlink"""
    link = fields.URL(schemes=['https'], required=True)
    linkText = fields.Str(required=True)


class StringorList(fields.Field):
    """Custom field to manage values that are lists but accept strings"""
    def _serialize(self, value, attr, obj, **kwargs):
        if isinstance(value, str):
            value = [value]
        if len(value) == 0 or not isinstance(value, list) and all(isinstance(s, str) for s in value):
            raise ValidationError("Family must be a nonempty string or a nonempty list of strings")
        return super()._serialize(value, attr, obj, **kwargs)

    def _deserialize(self, value, attr, data, **kwargs):
        if len(value) == 0:
            raise ValidationError("Family cannot be empty")
        if isinstance(value, list):
            return value
        else:
            return [value]

# PIPELINE: SERVICE:
class PipelineServiceSchema(Schema):
    """Pipeline Service Schema"""
    name = fields.Str(required=True)
    team = fields.Str(required=True)
    family = StringorList(required=True)
    serviceDiscoveryName = fields.Str()
    jiraProjectKey = fields.Str()
    pmContact = fields.Email()
    qaContact = fields.Email()
    gitDefaultBranch = fields.Str(required=False)


# PIPELINE: BUILD: EC2
def build_ec2_steps_disambiguation(object_dict: Any, _: Any, allow_directory: bool = True) -> Schema:
    """
    Disambiguation method used by marshmallow schema to fully validate Image Factory build steps
    :param object_dict: Object to disabiguate
    :param _: Parent of object for context
    :param allow_directory: Allows the API to not allow directory type, while config files do
    """
    obj_type = object_dict.get('type')
    if obj_type:
        if obj_type == 'file':
            return PipelineBuildEc2StepsFileSchema()
        if obj_type == 'bash':
            return PipelineBuildEc2StepsBashSchema()
        if obj_type == 'ansible':
            return PipelineBuildEc2StepsAnsibleSchema()
        if allow_directory and obj_type == 'directory':
            return PipelineBuildEc2StepsDirectorySchema()
        else:
            raise TypeError(f'Unknown Build Step Type: {obj_type}')
    else:
        raise TypeError('Build Steps require `type` value')


class PipelineBuildEc2EbsSchema(Schema):
    """Pipeline Build Ec2 EBS Schema"""
    deleteOnTermination = fields.Boolean()
    deviceName = fields.Str(required=True)
    volumeSize = fields.Integer()
    volumeType = fields.Str()


class PipelineBuildEc2StepsFileSchema(Schema):
    """Pipeline Build EC2 Steps File Schema"""
    type = fields.Str(required=True)
    source = fields.Str(required=True)
    logged = fields.Boolean(missing=True, default=True)
    destination = fields.Str(required=True)


class PipelineBuildEc2StepsDirectorySchema(Schema):
    """Pipeline Build EC2 Steps File Schema"""
    type = fields.Str(required=True)
    source = fields.Str(required=True)
    logged = fields.Boolean(missing=True, default=True)
    destination = fields.Str(required=True)


class PipelineBuildEc2StepsBashSchema(Schema):
    """Pipeline Build EC2 Steps File Schema"""
    type = fields.Str(required=True)
    commands = fields.List(fields.Str(), required=True)
    logged = fields.Boolean(missing=True, default=True)


class PipelineBuildEc2StepsAnsibleSchema(Schema):
    """Pipeline Build EC2 Steps File Schema"""
    type = fields.Str(required=True)
    extraVars = fields.Dict(keys=fields.Str(), values=fields.Str(), default={})
    playbook = fields.Str(required=True)
    inv = fields.Str(required=True)
    role = fields.Str()
    verbosity = fields.Integer(default=0, validate=Range(min=0, max=4))
    logged = fields.Boolean(missing=True, default=True)


class PipelineBuildEc2ApplicationPortProxySchema(Schema):
    """Pipeline Build EC2 Application Port Proxy Schema"""
    enabled = fields.Boolean(missing=True, default=True)
    routeTimeoutSeconds = fields.Integer(required=False, validate=Range(min=1))
    port = fields.Integer(required=False, validate=Range(min=SYSTEM_PORT_END, max=MAX_PORT))

    # pylint: disable=R0201 disable=W0613
    @validates_schema
    def validate_port(self, data, **kwargs):
        """To valid if proxy port is provided or not"""
        if data["enabled"] is True and "port" not in data.keys():
            raise ValidationError("proxy is enabled hence proxy port is required", "port")


class PipelineBuildEc2ApplicationPortSchema(Schema):
    """Pipeline Build EC2 Application Port Schema"""
    proxy = fields.Nested(PipelineBuildEc2ApplicationPortProxySchema())


class PipelineBuildEc2ApplicationSchema(Schema):
    """Pipeline Build EC2 Application Schema"""
    ports = fields.Dict(
        keys=fields.Integer(validate=Range(min=1, max=MAX_PORT)),
        values=fields.Nested(PipelineBuildEc2ApplicationPortSchema()),
        validate=Length(min=1),
        required=True
    )


# PIPELINE: SECURITY
class PipelineSecuritySchema(Schema):
    """Pipeline Security Schema"""
    stig = fields.Str(validate=validate.OneOf(valid_stigs))


class PipelinePostOptionsSchema(Schema):
    """Pipeline Post INstall Options Schema"""
    disablePackageManager = fields.Boolean()
    disableTmpExecutables = fields.Boolean()
    disableVarLogExecutables = fields.Boolean()
    disableNetrc = fields.Boolean()
    enableFips = fields.Boolean()

class OpenTelemetryEc2MemorySchema(Schema):
    """
        Memory Configuration for OpenTelemetry
        Docs: https://github.com/open-telemetry/opentelemetry-collector/tree/main/processor/memorylimiterprocessor
    """

    # pylint: disable=R0201
    @validates_schema
    def validate_spike_less_than_max(self, data, **_):
        """Validates that the spike percent is less than the max percentage"""
        max_percent = data.get('maxPercentage', OPEN_TELEMETRY_DEFAULT_MEMORY_MAX_PERCENTAGE)
        spike_percent = data.get('spikeLimitPercentage', OPEN_TELEMETRY_DEFAULT_MEMORY_SPIKE_PERCENTAGE)

        if max_percent < spike_percent :
            # pylint: disable=C0301
            raise ValidationError(f"maxPercentage {max_percent} can not be less than spikeLimitPercentage {spike_percent}")

    # Taken from Otel Doc:
    # Maximum amount of total memory targeted to be allocated by the process heap.
    # This configuration is supported on Linux systems with cgroups
    # and it's intended to be used in dynamic platforms like docker.
    # This option is used to calculate memory_limit from the total available memory.
    # For instance setting of 75% with the total memory of 1GiB will result in the limit of 750 MiB.
    # The fixed memory setting (limit_mib) takes precedence over the percentage configuration.
    maxPercentage = fields.Int(load_default = OPEN_TELEMETRY_DEFAULT_MEMORY_MAX_PERCENTAGE,
                               validate = Range(OPEN_TELEMETRY_MIN_MEMORY_MAX_PERCENTAGE,
                                                OPEN_TELEMETRY_MAX_MEMORY_MAX_PERCENTAGE))

    # Taken from Otel Doc:
    # ********************
    # Maximum spike expected between the measurements of memory usage.
    # The value must be less than limit_percentage.
    # This option is used to calculate spike_limit_mib from the total available memory.
    # For instance setting of 25% with the total memory of 1GiB will result in the spike limit of 250MiB.
    spikeLimitPercentage = fields.Int(load_default = OPEN_TELEMETRY_DEFAULT_MEMORY_SPIKE_PERCENTAGE,
                                      validate = Range(OPEN_TELEMETRY_MIN_MEMORY_SPIKE_PERCENTAGE,
                                                       OPEN_TELEMETRY_MAX_MEMORY_SPIKE_PERCENTAGE))

class OpenTelemetryEc2PortsSchema(Schema):
    """Ports Configuration allows user to change the ports that otel uses"""
    # Port that the otel collector runs on
    collector = fields.Int(load_default = OPEN_TELEMETRY_DEFAULT_HTTP_COLLECTOR_PORT,
                           validate=Range(min=SYSTEM_PORT_END, max=MAX_PORT))
    # Port that otel will generate its own Prometheus Metrics
    collectorMetrics = fields.Int(load_default = OPEN_TELEMETRY_DEFAULT_COLLECTOR_METRICS_PORT,
                                  validate=Range(min=SYSTEM_PORT_END, max=MAX_PORT))

class OpenTelemetryTracesAttributes(Schema):
    """
        Trace Attribute configuration
        Currently supported actions:
            * redact - redacts a specific attribute to ****
            * remove - removes a specific attribute from all traces
    """
    action = fields.Str(validate=OneOf(['redact', 'remove']))


class OpenTelemetryTracesSchema(Schema):
    """Open Telemetry Traces Configuration"""
    attributes = fields.Dict(keys=fields.Str(), values=fields.Nested(OpenTelemetryTracesAttributes()))


class OpenTelemetryEc2Schema(Schema):
    """Open Telemetry Configuration for Ec2"""
    enabled = fields.Bool(load_default=False)
    memory = fields.Nested(OpenTelemetryEc2MemorySchema(),
                           load_default=lambda: OpenTelemetryEc2MemorySchema().load({}))
    ports = fields.Nested(OpenTelemetryEc2PortsSchema(),
                          load_default=lambda: OpenTelemetryEc2PortsSchema().load({}))
    traces = fields.Nested(OpenTelemetryTracesSchema(),
                           load_default=lambda: OpenTelemetryTracesSchema().load({}))


class PipelineBuildEc2Schema(Schema):
    """Pipeline Build Ec2 Schema"""

    # This will affect the ApiBuildEc2Schema below
    # pylint: disable=R0201
    @validates_schema
    def validate_ami(self, data: Dict[str, Any], **__):
        """Validates that either amiId or amiVersion is set, but not both"""
        if 'amiVersion' in data and 'amiId' in data:
            raise ValidationError('amiVersion and amiId are mutually exclusive')

        if not ('amiVersion' in data or 'amiId' in data):
            raise ValidationError('amiVersion or amiId must be set')

    steps = fields.List(
        PolyField(
            serialization_schema_selector=build_ec2_steps_disambiguation,
            deserialization_schema_selector=build_ec2_steps_disambiguation,
            required=True
        )
    )

    amiVersion = fields.Str()
    amiId = fields.Str()
    postInstallOptionsDefault = fields.Boolean()
    postInstallOptions = fields.Nested(PipelinePostOptionsSchema())
    security = fields.Nested(PipelineSecuritySchema())
    ebs = fields.List(
        fields.Nested(PipelineBuildEc2EbsSchema)
    )
    amiTags = fields.Dict(
        keys=fields.Str(
            validate=validate.Length(min=1, max=128)
        ),
        values=fields.Str(
            validate=validate.Length(min=0, max=256)
        )
    )
    telemetry = fields.Nested(
        OpenTelemetryEc2Schema(),
        load_default=lambda: OpenTelemetryEc2Schema().load({}))
    applications = fields.Dict(
        keys=fields.String(validate=Length(min=1, max=60)),
        values=fields.Nested(PipelineBuildEc2ApplicationSchema(), required=True)
    )


# PIPELINE: BUILD
class PipelineBuildSchema(Schema):
    """Pipeline Build Schema"""
    ec2 = fields.Nested(PipelineBuildEc2Schema)


# PIPELINE: DEPLOY (skynet)
class DeployCmSchema(Schema):
    """Dploy Change Management Schema"""
    submit = fields.Bool(missing=False, required=False)
    finish = fields.Bool(missing=True, required=False)


class PreDeployServiceTestSchema(Schema):
    """Pre Deploy Service Test Schema"""
    name = fields.Str(required=True)


class PostDeployServiceTestSchema(Schema):
    """Post Deploy Service Test Schema"""
    type = fields.Str(validate=validate.OneOf([SERVICE_TESTS_UNIFIED, SERVICE_TESTS_LEGACY]),
                      missing=SERVICE_TESTS_UNIFIED, required=False)
    name = fields.Str(required=True)


class DeployServiceTestsSchema(Schema):
    """Deploy Service Tests Schema"""
    preDeploy = fields.Nested(PreDeployServiceTestSchema, required=False)
    postDeploy = fields.Nested(PostDeployServiceTestSchema, required=False)


class DeployScheduleEntrySchema(Schema):
    """An individual entry in a schedule config"""
    start = fields.Str(example='10:00')
    stop = fields.Str(example='14:00')
    days = fields.List(fields.Str(example=['monday'], validate=validate.OneOf(DAYS_OF_WEEK)))
    timezone = fields.Str(example='UTC', validate=validate.OneOf(pytz.all_timezones), required=False)


class DeployScheduleSchema(Schema):
    """The top level container for schedule entries"""
    name = fields.Str(required=False)
    sortKey = fields.Int(required=False)
    schedules = fields.Nested(DeployScheduleEntrySchema, many=True, required=False)

class DeployTerraformProvidersSchema(Schema):
    """Terraform provider Schema"""
    version = fields.Str(validate=validate.Regexp(SEM_VER_REGEX), required=True)

class DeployTerraformSchema(Schema):
    """Top Level container for specifying terraform version and provider version"""
    version = fields.Str(validate=validate.Regexp(SEM_VER_REGEX), required=False)
    providers = fields.Dict(
        keys=fields.Str(validate=validate.OneOf(valid_providers)),
        values=fields.Nested(DeployTerraformProvidersSchema), required=False
    )


class HabitatDeployTypeSchema(Schema):
    """
    Schema for Skynet habitat type of config
    """
    type = fields.Str(required=True)
    name = fields.Str(required=True)
    schedule = fields.Str(required=True)


class EnvironmentDeployTypeSchema(Schema):
    """
     Schema for Skynet environment type of config. Equivalent of dev-*/dev-||
    """
    type = fields.Str(required=True)
    name = fields.Str(required=True)
    sequence = fields.Str(validate=validate.OneOf(VALID_SEQUENCE_TYPES), required=True)
    schedule = fields.Str(required=False, default=DEFAULT_SCHEDULE)
    filters = fields.Dict(keys=fields.Str(), values=fields.Raw(), required=False)

    @validates('filters')
    def validate_filters(self, filters):  # pylint: disable=no-self-use
        """Validate the filters type"""
        if not filters:
            return

        for key, value in filters.items():
            if not isinstance(value, (str, int, float, bool)):
                raise ValidationError(f'invalid filter type for {key}: {value}; only primitive types are allowed')


def deploy_schema_generator(base_object, _):
    """
    :param base_object: base config that has the type
    :param _:
    :return: The schema to validate against based off the type
    """
    deploy_type = base_object.get("type")

    if deploy_type == HABITAT_TYPE_DEPLOY:
        return HabitatDeployTypeSchema()
    elif deploy_type == ENVIRONMENT_TYPE_DEPLOY:
        return EnvironmentDeployTypeSchema()
    else:
        raise ValidationError(f'Invalid deploy type- {deploy_type} found. Valid types {VALID_TYPES_FOR_PATH}')


class DeploySchema(Schema):
    """Deploy Schema"""
    path = fields.List(PolyField(
        serialization_schema_selector=deploy_schema_generator,
        deserialization_schema_selector=deploy_schema_generator), required=False)
    branchPath = fields.List(PolyField(
        serialization_schema_selector=deploy_schema_generator,
        deserialization_schema_selector=deploy_schema_generator), required=False)
    changeManagement = fields.Nested(DeployCmSchema, required=False)
    serviceTests = fields.Nested(DeployServiceTestsSchema, required=False)
    schedules = fields.Dict(keys=fields.Str(), values=fields.Nested(DeployScheduleSchema), required=False)
    terraform = fields.Nested(DeployTerraformSchema, required=False)

# Service Operation Schemas

class GenesysPropertiesOperationSchema(Schema):
    """ Genesys Properties Schema"""
    path = fields.Str(required=True)

class AppManagementOperationSchema(Schema):
    """ App Management Schema"""
    configKey = fields.Str(required=True)


class KingpinDynamicProperties(Schema):
    """ Schema for the v1 Dynamic Properties operation """
    appManagement = fields.Nested(AppManagementOperationSchema)


class AdrestiaCommandBase(Schema):
    """ Base schema for Adrestia operations """
    description = fields.Str(required=False)


class KingpinAdrestiaOperations(Schema):
    """ Schema for the Adrestia operations """
    requestResources = fields.Nested(AdrestiaCommandBase)
    modifyInstanceState = fields.Nested(AdrestiaCommandBase)
    shutdownInstance = fields.Nested(AdrestiaCommandBase)
    addOrgToBlackList = fields.Nested(AdrestiaCommandBase)
    removeOrgFromBlackList = fields.Nested(AdrestiaCommandBase)


def validate_cloudformation_type(cf_type):
    """ Validates the cloudformation type"""
    if not cf_type:
        return

    parts = cf_type.split('::')
    if len(parts) != 3:
        raise ValidationError(f'invalid cloudformation type: {cf_type}')

    if parts[0] not in CLOUDFORMATION_NAME_SPACES:
        raise ValidationError(f'invalid cloudformation type: {cf_type}; must start with one of '
                              f'{CLOUDFORMATION_NAME_SPACES}')

    if parts[0] == 'Genesys' and cf_type not in GENESYS_CLOUDFORMATION_TYPES:
        raise ValidationError(f'invalid genesys cloudformation type: {cf_type}; must be one of '
                              f'{GENESYS_CLOUDFORMATION_TYPES}')


class CustomLambdaStringPropertySchema(Schema):
    """ Schema for kingpin string properties """
    # Base fields
    type = fields.Str(required=True)
    required = fields.Bool(required=False)
    setting = fields.Bool(required=False)
    hidden = fields.Bool(required=False)
    displayName = fields.Str(required=False)
    description = fields.Str(required=False)
    placeholder = fields.Str(required=False)
    # Specific to string
    enum = fields.List(fields.Str(required=True), required=False)
    defaultValue = fields.Str(required=False)
    cloudformationType = fields.Str(required=False)

    @validates('cloudformationType')
    def validate_cf_type(self, cf_type):  # pylint: disable=no-self-use
        """Validate the cloudformation type"""
        validate_cloudformation_type(cf_type)


class CustomLambdaJsonPropertySchema(Schema):
    """ Schema for kingpin JSON properties """
    # Base fields
    type = fields.Str(required=True)
    required = fields.Bool(required=False)
    setting = fields.Bool(required=False)
    hidden = fields.Bool(required=False)
    displayName = fields.Str(required=False)
    description = fields.Str(required=False)
    placeholder = fields.Str(required=False)
    # Specific to JSON
    defaultValue = fields.Dict(required=False)


class CustomLambdaBooleanPropertySchema(Schema):
    """ Schema for kingpin boolean properties """
    # Base fields
    type = fields.Str(required=True)
    setting = fields.Bool(required=False)
    hidden = fields.Bool(required=False)
    displayName = fields.Str(required=False)
    description = fields.Str(required=False)
    placeholder = fields.Str(required=False)
    # Specific to boolean
    defaultValue = fields.Bool(required=False)


def validate_default_between_min_and_max(data):
    """ validates that if there is default value, it's between any specified min and max values"""
    if ('min' in data) and ('defaultValue' in data) and (data['defaultValue'] < data['min']):
        raise ValidationError(f"The defaultValue ({data['defaultValue']}) is less than min ({data['min']})")

    if ('max' in data) and ('defaultValue' in data) and (data['defaultValue'] > data['max']):
        raise ValidationError(f"The defaultValue ({data['defaultValue']}) is greater than max ({data['max']})")


class CustomLambdaIntegerPropertySchema(Schema):
    """ Schema for kingpin integer properties """
    # Base fields
    type = fields.Str(required=True)
    required = fields.Bool(required=False)
    setting = fields.Bool(required=False)
    hidden = fields.Bool(required=False)
    displayName = fields.Str(required=False)
    description = fields.Str(required=False)
    placeholder = fields.Str(required=False)
    # Specific to number
    min = fields.Int(required=False)
    max = fields.Int(required=False)
    defaultValue = fields.Int(required=False)

    @validates_schema
    # pylint: disable=W0613
    # pylint: disable=R0201
    def validate_default_value(self, data, **kwargs):
        """ validates that if there is default value, it's between any specified min and max values"""
        validate_default_between_min_and_max(data)


class CustomLambdaNumberPropertySchema(Schema):
    """ Schema for kingpin number properties """
    # Base fields
    type = fields.Str(required=True)
    required = fields.Bool(required=False)
    setting = fields.Bool(required=False)
    hidden = fields.Bool(required=False)
    displayName = fields.Str(required=False)
    description = fields.Str(required=False)
    placeholder = fields.Str(required=False)
    # Specific to number.
    min = fields.Number(required=False)
    max = fields.Number(required=False)
    defaultValue = fields.Number(required=False)

    @validates_schema
    # pylint: disable=W0613
    # pylint: disable=R0201
    def validate_default_value(self, data, **kwargs):
        """ validates that if there is default value, it's between any specified min and max values"""
        validate_default_between_min_and_max(data)


class CustomLambdaPropertiesArrayItemsSchema(Schema):
    """ Schema for Custom Lambda Array items """
    type = fields.Str(required=True)
    enum = fields.List(fields.Str(required=True), required=False)
    cloudformationType = fields.Str(required=False)

    @validates('cloudformationType')
    def validate_cf_type(self, cf_type):  # pylint: disable=no-self-use
        """Validate the cloudformation type"""
        validate_cloudformation_type(cf_type)


class CustomLambdaArrayPropertySchema(Schema):
    """Kingpin Array Property Schema"""
    # Base fields
    type = fields.Str(required=True)
    required = fields.Bool(required=False)
    setting = fields.Bool(required=False)
    hidden = fields.Bool(required=False)
    displayName = fields.Str(required=False)
    description = fields.Str(required=False)
    placeholder = fields.Str(required=False)
    # Specific to array
    items = fields.Nested(CustomLambdaPropertiesArrayItemsSchema, required=True)
    defaultValue = fields.List(fields.Str(required=False), required=False)


def custom_lambda_properties_disambiguation(object_dict: Any, _: Any) -> Schema:
    """
    Disambiguation method used by marshmallow schema to validate Kingpin properties
    :param object_dict: Object to disambiguate
    :param _: Parent of object for context
    """
    # WARNING: if you add a new type here, you might need to update the Union in
    # KingpinOperationExtensions too.
    obj_type = object_dict.get('type')
    if obj_type:
        if obj_type == 'string':
            return CustomLambdaStringPropertySchema()
        if obj_type == 'json':
            return CustomLambdaJsonPropertySchema()
        if obj_type == 'boolean':
            return CustomLambdaBooleanPropertySchema()
        if obj_type == 'integer':
            return CustomLambdaIntegerPropertySchema()
        if obj_type == 'number':
            return CustomLambdaNumberPropertySchema()
        if obj_type == 'array':
            return CustomLambdaArrayPropertySchema()
        raise TypeError('Custom Lambda unknown properties `type`')

    raise TypeError('Custom Lambda properties require `type` value')


class CustomLambdaPayload(Schema):
    """ Schema for Custom Lambda payload properties """
    # Order gets overwritten by the @post_load method below. Don't bother specifying it in your configuration.
    order = fields.List(fields.Str(), required=False)
    properties = fields.Dict(keys=fields.Str(),
                             values=PolyField(
                                 serialization_schema_selector=custom_lambda_properties_disambiguation,
                                 deserialization_schema_selector=custom_lambda_properties_disambiguation)
                             )

    # Python dictionaries are ordered by default since 3.7. However, we don't want to depend
    # on whatever might be receiving this on the other side of an API call to get it right.
    # So we inject the order field to ensure the fields get displayed properly.
    @post_load
    # pylint: disable=R0201,W0613
    def inject_order(self, data, **kwargs):
        """Automatically create and populate the order field"""
        return {
            'order': list(data['properties'].keys()),
            'properties': data['properties']
        }


def response_schema_validator(response_schema):
    """Ensure response schema is a valid schema definition"""
    try:
        Draft202012Validator.check_schema(response_schema)
    except SchemaError as err:
        json_path = err.json_path.replace('$', 'responseSchema')
        raise ValidationError(f'{err.message} at {json_path}') from err


class KingpinCustomLambdaOperations(Schema):
    """ Base schema for Custom Lambda top level fields """
    # serviceExecuteRole is being deprecated; the first step is to make it optional
    serviceExecuteRole = fields.Str(required=False)
    description = fields.Str()
    documentation = fields.List(fields.Nested(Hyperlink))
    displayName = fields.Str()
    accessTypes = fields.List(fields.Str(validate=validate.OneOf(CUSTOM_LAMBDA_ACCESS_TYPES)))
    payload = fields.Nested(CustomLambdaPayload)
    responseSchema = fields.Raw(required=False)

    @validates('responseSchema')
    def validate_response_schema(self, response_schema):  # pylint: disable=no-self-use
        """Ensure response schema is a valid schema definition"""
        response_schema_validator(response_schema)


class KingpinOperationExtensions(Schema):
    """ Base schema for Custom Lambda operation extensions """
    description = fields.Str()
    documentation = fields.List(fields.Nested(Hyperlink))
    displayName = fields.Str()
    extends = fields.Str()
    # The fields in the Union match those in the payload properties.
    # Refer to custom_lambda_properties_disambiguation.
    settings = fields.Dict(keys=fields.Str(), values=Union([
        fields.Bool(),
        fields.Str(),
        fields.Integer(),
        fields.Number(),
        fields.List(fields.Str()),
        fields.Dict()
    ]))
    payload = fields.Nested(CustomLambdaPayload)
    responseSchema = fields.Raw(required=False)

    @validates('responseSchema')
    def validate_response_schema(self, response_schema):  # pylint: disable=no-self-use
        """Ensure response schema is a valid schema definition"""
        response_schema_validator(response_schema)


class KingpinSchema(Schema):
    """ Main schema for the operations config """
    dynamicProperties = fields.Nested(KingpinDynamicProperties)
    genesysProperties = fields.Nested(GenesysPropertiesOperationSchema)
    adrestiaOperations = fields.Nested(KingpinAdrestiaOperations)
    customLambdas = fields.Dict(keys=fields.Str(), values=fields.Nested(KingpinCustomLambdaOperations))
    operationNames = fields.List(fields.Str(required=True))
    operationExtensions = fields.Dict(keys=fields.Str(), values=fields.Nested(KingpinOperationExtensions))

    @validates('operationNames')
    def no_duplicate_operation_names(self, operation_names):  # pylint: disable=no-self-use
        """Ensure no duplicate operation names"""
        if len(operation_names) != len(set(operation_names)):
            raise ValidationError(f'operationNames contains duplicates: {operation_names}')

    @validates_schema
    # pylint: disable=W0613
    # pylint: disable=R0201
    def validate_operation_name_uniqueness(self, data, **kwargs):
        """ validates that there are no duplications between names and extensions """
        operation_names = data.get('operationNames') or []
        operation_extensions = data.get('operationExtensions') or {}
        operation_extension_names = operation_extensions.keys()
        for operation_name in operation_names:
            if operation_name in operation_extension_names:
                raise ValidationError('Operation names must be unique across operationNames and '
                                      f'operationExtensions. Having {operation_name} in both is forbidden.')


# PIPELINE: COSTALLOCATION:
class PipelineCostAllocationSchema(Schema):
    """Pipeline Cost Tracking Schema"""
    productGroup = fields.Str(validate=validate.OneOf(valid_product_groups))
    businessUnit = fields.Str(validate=validate.OneOf(valid_business_units))
    consumedBy = fields.List(fields.Str(validate=validate.OneOf(valid_business_units)))

    @validates(field_name='consumedBy')
    def check_list_for_unique(self, data, **_):  # pylint: disable=no-self-use
        """Validates a unique list"""
        if len(set(data)) != len(data):
            raise ValidationError(f'consumedBy contains duplicate values {data}')


# Static Asset Pipeline Schemas

class StaticAssetSnsPushAction(Schema):
    """
    SNS-push action schema
    urlTtl is in seconds
    """
    ONE_WEEK = 604800

    # pylint: disable=R0903
    class Meta:
        """Meta.include allows us to have fields that are python reserved keywords."""
        include = {
            'type': fields.Str(required=True)
        }

    logicalTopicName = fields.Str(required=True)
    urlTtl = fields.Int(validate=validate.Range(max=ONE_WEEK))


class StaticAssetS3CopyMetadataSystemDefined(Schema):
    """
    S3 system defined metadata information
    """
    contentType = fields.Str()
    cacheControl = fields.Str()
    contentDisposition = fields.Str()
    contentEncoding = fields.Str()
    contentLanguage = fields.Str()
    expires = fields.Str()
    xAmzWebsiteRedirectLocation = fields.Str()


class StaticAssetS3CopyMetadata(Schema):
    """
    S3 metadata information
    """
    system = fields.Nested(StaticAssetS3CopyMetadataSystemDefined)


class StaticAssetS3CopyAction(Schema):
    """
    S3-copy action schema
    """

    # pylint: disable=R0903
    class Meta:
        """Meta.include allows us to have fields that are python reserved keywords."""
        include = {
            'type': fields.Str(required=True)
        }

    logicalBucketName = fields.Str(required=True)
    prefix = fields.Str()
    metadata = fields.Nested(StaticAssetS3CopyMetadata)


def action_schema_serialization_disambiguation(base_object, _):
    """Selects the action schema to use based on the 'type' field of the action"""
    action_type = base_object.get("type")

    if action_type == STATIC_ASSETS_SNS_PUSH:
        return StaticAssetSnsPushAction

    if action_type == STATIC_ASSETS_S3_COPY:
        return StaticAssetS3CopyAction

    raise ValidationError(
        f'Action \'type\': \'{action_type}\' does not match one of the allowable actions: '
        f'{STATIC_ASSETS_ACTIONS}. Action types are case sensitive')


class StaticAssetsEnvSchema(Schema):
    """Common schema for promote and release"""
    stackName = fields.Str()
    actions = fields.List(PolyField(
        serialization_schema_selector=action_schema_serialization_disambiguation,
        deserialization_schema_selector=action_schema_serialization_disambiguation
    ))


class StaticAssetSchema(Schema):
    """ Schema for the an individual static asset in the unified config"""

    src = fields.Str(required=True)
    srcType = fields.Str(validate=validate.OneOf(STATIC_ASSETS_SRC_TYPES))
    release = fields.Dict(keys=fields.Str(), values=fields.Nested(StaticAssetsEnvSchema), required=False)
    promote = fields.Dict(keys=fields.Str(), values=fields.Nested(StaticAssetsEnvSchema), required=False)

    @validates_schema
    # pylint: disable=W0613
    # pylint: disable=R0201
    def validate_promote_type(self, data, **kwargs):
        """ validates that there is at least one of release / promote defined in the config"""
        if ('release' not in data) and ('promote' not in data):
            raise ValidationError("A release or a promote step will need to be defined for static assets")

    @validates_schema
    # pylint: disable=W0613
    # pylint: disable=R0201
    def validate_zip_src_type(self, data, **kwargs):
        """ validates that when srcType is zip, the src extension is .zip"""
        src = data['src']
        src_type = data.get('srcType') or 'file'
        if src_type == 'zip' and not src.endswith('.zip'):
            raise ValidationError(f'The srcType is {src_type} but {src} does not end with a .zip extension')


class StaticAssetsContentTypeConversion(Schema):
    """ Schema for static assets content type conversion used during S3-copy"""
    default = fields.Str()
    fileExtensionMapping = fields.Dict(keys=fields.Str(), values=fields.Str(), required=True)


def get_actions_from_config(config):
    """
    :param config: The asset config
    :return: generator which is a list of lists, each list being an action
    """
    for field in ['promote', 'release']:
        if field in config:
            field_config = config[field].values()
            for val in field_config:
                yield val['actions']


class StaticAssetsSchema(Schema):
    """ Schema for the dictionary of static assets in the unified config"""
    processingOrder = fields.List(fields.Str())
    contentTypeConversion = fields.Nested(StaticAssetsContentTypeConversion)
    assets = fields.Dict(keys=fields.Str(), values=fields.Nested(StaticAssetSchema), required=True)

    @validates_schema
    # pylint: disable=W0613
    # pylint: disable=R0201
    def validate_processing_order(self, data, **kwargs):
        """ Validate that processing order makes sense relative to the dictionary of static assets """
        processing_order = data.get("processingOrder")
        if processing_order:
            asset_names = list(data['assets'].keys())
            if set(processing_order) != set(asset_names):
                raise ValidationError(f'processing order: {str(processing_order)} must include all '
                                      f'asset names: {str(asset_names)} and nothing extra')


class PipelineArtifactsSchema(Schema):
    """ Main schema for static asset artifacts"""
    staticAssets = fields.Nested(StaticAssetsSchema)


# PIPELINE
class PipelineParametersSchema(Schema):
    """Config Parameters"""
    default = fields.Raw()
    habitats = fields.Dict(keys=fields.Str(), values=fields.Raw())


class BuildMetadataJenkinsSchema(Schema):
    """Jenkins build metadata"""
    buildCause = fields.String()
    buildNumber = fields.String()
    buildUrl = fields.String()
    jobName = fields.String()
    jobUrl = fields.String()
    nodeLabels = fields.List(fields.String())
    nodeName = fields.String()
    changesUrl = fields.String()
    displayUrl = fields.String()
    workspace = fields.String()

class BuildMetadataGitSchema(Schema):
    """Git build metadata"""
    branch = fields.String()
    commitHash = fields.String()
    url = fields.String()
    defaultBranch = fields.Boolean()

class BuildMetadataSchema(Schema):
    """Metadata gathered at build time"""
    jenkins = fields.Nested(BuildMetadataJenkinsSchema())
    git = fields.Nested(BuildMetadataGitSchema())
    buildTime = fields.Integer()

class PipelineSchema(Schema):
    """Pipeline Schema"""
    parameters = fields.Dict(keys=fields.Str(), values=fields.Nested(PipelineParametersSchema()))
    service = fields.Nested(PipelineServiceSchema(), required=True)
    build = fields.Nested(PipelineBuildSchema())
    deploy = fields.Nested(DeploySchema())
    artifacts = fields.Nested(PipelineArtifactsSchema())
    operations = fields.Nested(KingpinSchema())
    costAllocation = fields.Nested(PipelineCostAllocationSchema())
    buildMetadata = fields.Nested(BuildMetadataSchema())


class PreRenderSchema(Schema):
    """Pre Render Schema"""
    parameters = fields.Dict(keys=fields.Str(), values=fields.Nested(PipelineParametersSchema()))
    service = fields.Raw()
    build = fields.Raw()
    deploy = fields.Raw()
    artifacts = fields.Raw()
    operations = fields.Raw()
    costAllocation = fields.Raw()
    buildMetadata = fields.Raw()


# Image Factory API Config Schema
def api_ec2_steps_disambiguation(object_dict: Any, parent_object_dict: Any) -> Schema:
    """
    Disambiguation method used by marshmallow schema to fully validate Image Factory build steps
    :param object_dict: Object to disabiguate
    :param parent_object_dict: Parent of object for context
    """
    return build_ec2_steps_disambiguation(object_dict, parent_object_dict, allow_directory=False)


class ApiBuildEc2Schema(PipelineBuildEc2Schema):
    """Api Build Ec2 Schema"""
    # See other fields in PipelineBuildEc2Schema above!
    steps = fields.List(
        PolyField(
            serialization_schema_selector=api_ec2_steps_disambiguation,
            deserialization_schema_selector=api_ec2_steps_disambiguation,
            required=True
        )
    )
