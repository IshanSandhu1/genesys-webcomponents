"""Genesys Pipeline Unified Config Parser"""

from pathlib import Path
from copy import deepcopy
from typing import Iterator

from jsonpointer import JsonPointer

import dpath.util
import yaml

from .exceptions import ServiceNameValidationException, RepoPathOrMergedConfigValidationException, \
    RepoPathOrMergedExclusiveConfigValidationException, NoConfigFoundException

from .schema import PipelineSchema, PreRenderSchema


# pylint: disable=R0201
class UnifiedConfig:
    """
    Unified Config Parser

    Unified parser for all things Genesys Pipeline.
    All configuration must be located in either the `pipeline.yml` or `pipeline.d/*.yml|json`
    files. This parser will perform a deep merge on the identified files.

    Repo must have at least a `pipeline.yml` or a yaml|json file located in the `pipeline.d` directory.

    Files in the pipeline.d directory have precidence over pipeline.yml, and files in pipeline.d later in
    the alphabet have precidence over files with an earlier file name.

    The `pipeline.yml` and `pipeline.d` files must be located at the root of the repository.
    """

    def __init__(self, *, repo_path: str = None, merged_config: dict = None) -> None:
        """
        :param repo_path: Path to root of repo
        """
        if repo_path is None and merged_config is None:
            raise RepoPathOrMergedConfigValidationException('UnifiedConfig requires either repo_path or merged_config')

        if repo_path is not None and merged_config is not None:
            raise RepoPathOrMergedExclusiveConfigValidationException(
                'UnifiedConfig requires either repo_path or merged_config, but not both'
            )
        self.schema = PipelineSchema()
        self.pre_render_schema = PreRenderSchema()

        # If loading from disk
        if repo_path:
            self.repo_path = Path(repo_path)

        # If loading from pre-merged dict
        self.merged_config = None
        if merged_config:
            self.merged_config = self.pre_render_schema.load(merged_config)

        self.file_types = [
            'yml',
            'yaml',
            'json',
        ]

        self.functions = {
            'Ref': self._process_ref
        }

    def get_merged(self) -> dict:
        """
        Returns un-resolved merged template with Ref's still in place
        :return: merged config
        """

        if self.merged_config is None:
            merged: dict = {}
            for contents in self._parse_files():
                dpath.util.merge(merged, contents)

            self.merged_config = self.pre_render_schema.load(merged)
        return self.merged_config

    def get_referencealbe_habitats(self):
        """
        Returns all habitat names referenced in the parameters
        :return: list of habitat names
        """
        merged = self.get_merged()
        habitats = set()
        for _, parameter_data in merged.get('parameters', {}).items():
            habitats.update(set(parameter_data.get('habitats', {}).keys()))
        return list(habitats)


    def get_config(self, *, habitat='default') -> dict:
        """
        Generates a unified config files from all pipeline config files.
        :return: unified config
        """
        merged = deepcopy(self.get_merged())
        merged = self._process_functions(
            data=merged,
            parameters=self._parameters(merged),
            path=[],
            habitat=habitat
        )
        merged = self.schema.load(merged)
        return merged

    def validate_cluster_name_service_name(self, service_name, cluster_name):
        """
        checks if servicename and clustername matches else throws ServiceNameValidationException exception
        """
        if service_name != cluster_name:
            raise ServiceNameValidationException(
                f"serviceName '{service_name}' is not same as clusterName '{cluster_name}'")

    def _enumerate_files(self) -> Iterator[Path]:
        file_found = False
        sub_directory = self.repo_path / 'pipeline.d'
        if sub_directory.exists():
            sub_files = []
            for file_type in self.file_types:
                for sub_file in sub_directory.glob(f'*.{file_type}'):
                    sub_files.append(sub_file)

            for sub_file in sorted(sub_files):
                file_found = True
                yield sub_file
        if not file_found:
            raise NoConfigFoundException(f'Could not locate any config files in path: {self.repo_path}')

    def _parameters(self, data: dict):
        return data.get('parameters', {})

    def _parse_files(self) -> Iterator[dict]:
        for path in self._enumerate_files():
            with path.open() as fin:
                yield yaml.safe_load(fin)

    def _is_function(self, data) -> bool:
        if not isinstance(data, dict):
            return False
        if len(data) > 1:
            return False
        for key, _ in data.items():
            if key in self.functions:
                return True
        return False

    def _process_function(self, data: dict, parameters: dict, path: list, habitat: str):
        function_name = list(data.keys()).pop()
        return self.functions[function_name](
            data=data,
            parameters=parameters,
            path=path,
            habitat=habitat,
        )

    def _process_ref(self, data: dict, parameters: dict, path: list, habitat: str):
        name = data['Ref']
        if name in parameters:
            if habitat in parameters[name].get('habitats', {}):
                return parameters[name]['habitats'][habitat]
            else:
                return parameters[name]['default']
        json_path = JsonPointer.from_parts(path)
        raise Exception(f'Unknown Ref {name} at {json_path.path}')

    def _process_functions(self, data: dict, parameters: dict, path: list, habitat: str) -> dict:
        while self._is_function(data):
            data = self._process_function(
                data=data,
                parameters=parameters,
                path=path,
                habitat=habitat,
            )

        if isinstance(data, list):
            result = []
            for index, item in enumerate(data):
                local_path = path + [index]
                result.append(self._process_functions(
                    data=item,
                    parameters=parameters,
                    path=local_path,
                    habitat=habitat,
                ))
        elif isinstance(data, dict):
            result = {}
            for item_name, item in data.items():
                local_path = path + [item_name]
                result[item_name] = self._process_functions(
                    data=item,
                    parameters=parameters,
                    path=local_path,
                    habitat=habitat,
                )
        else:
            result = deepcopy(data)

        return result
